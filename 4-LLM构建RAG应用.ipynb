{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请你将由三个反引号分割的文本翻译成英文！text: ```Babylon是一个开源的JavaScript解析器和代码转换工具，用于分析和转换JavaScript代码。它是由Microsoft开发的，旨在提供一个高性能、可扩展和可靠的解析器，用于支持各种JavaScript工具和框架。```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 原生构建提示词模版\n",
    "template = \"\"\"请你将由三个反引号分割的文本翻译成英文！\\\n",
    "text: ```{text}```\n",
    "\"\"\"\n",
    "# 用户输入\n",
    "text = \"Babylon是一个开源的JavaScript解析器和代码转换工具，用于分析和转换JavaScript代码。\"\n",
    "# prompt打包\n",
    "prompt = template.format(text=text)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='你当前是一个翻译助手，请将 中文 翻译成 英文.'), HumanMessage(content='翻译内容：Babylon是一个开源的JavaScript解析器和代码转换工具，用于分析和转换JavaScript代码。')]\n",
      "---------\n",
      "你当前是一个翻译助手，请将 中文 翻译成 英文.\n"
     ]
    }
   ],
   "source": [
    "# LangChain实现\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "# system prompt template\n",
    "template = \"你当前是一个翻译助手，请将 {input_language} 翻译成 {output_language}.\"\n",
    "\n",
    "\n",
    "human_template = \"翻译内容：{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "\n",
    "text = \"Babylon是一个开源的JavaScript解析器和代码转换工具，用于分析和转换JavaScript代码。\"\n",
    "messages  = chat_prompt.format_messages(input_language=\"中文\", output_language=\"英文\", text=text)\n",
    "print(messages)\n",
    "print(\"---------\")\n",
    "print(messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-core in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.2.9)\n",
      "Collecting langchain-core\n",
      "  Downloading langchain_core-0.2.10-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: langchain-mistralai in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.1.8)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/codespace/.local/lib/python3.10/site-packages (from langchain-core) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain-core) (0.1.81)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/codespace/.local/lib/python3.10/site-packages (from langchain-core) (24.0)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain-core) (2.7.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from langchain-core) (8.3.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.2 in /home/codespace/.local/lib/python3.10/site-packages (from langchain-mistralai) (0.27.0)\n",
      "Requirement already satisfied: httpx-sse<1,>=0.3.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain-mistralai) (0.4.0)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain-mistralai) (0.19.1)\n",
      "Requirement already satisfied: anyio in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.25.2->langchain-mistralai) (4.4.0)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.25.2->langchain-mistralai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.25.2->langchain-mistralai) (1.0.5)\n",
      "Requirement already satisfied: idna in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.25.2->langchain-mistralai) (3.7)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.25.2->langchain-mistralai) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.25.2->langchain-mistralai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/codespace/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core) (3.10.5)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core) (2.32.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/codespace/.local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core) (4.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tokenizers<1,>=0.15.1->langchain-mistralai) (0.23.4)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (2024.5.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (4.66.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core) (2.0.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/codespace/.local/lib/python3.10/site-packages (from anyio->httpx<1,>=0.25.2->langchain-mistralai) (1.2.1)\n",
      "Downloading langchain_core-0.2.10-py3-none-any.whl (332 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.8/332.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: langchain-core\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.2.9\n",
      "    Uninstalling langchain-core-0.2.9:\n",
      "      Successfully uninstalled langchain-core-0.2.9\n",
      "Successfully installed langchain-core-0.2.10\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langchain-core langchain-mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<httpx.Client object at 0x77bb18b0e590> async_client=<httpx.AsyncClient object at 0x77bb18bf4700> mistral_api_key=SecretStr('**********')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I can help you translate that from Chinese to English.\n",
      "\n",
      "The translation is: \"Babylon is an open-source JavaScript parser and code transform tool, used for analyzing and transforming JavaScript code.\"\n"
     ]
    }
   ],
   "source": [
    "# 运行测试\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "# 读取本地/项目的环境变量。\n",
    "# find_dotenv()寻找并定位.env文件的路径\n",
    "# load_dotenv()读取该.env文件，并将其中的环境变量加载到当前的运行环境中  \n",
    "_ = load_dotenv(find_dotenv()); # 如果环境变量是全局的，这行代码可以省略\n",
    "\n",
    "# 获取环境变量 OPENAI_API_KEY\n",
    "api_key = os.environ['MISTRAL_API_KEY']\n",
    "\n",
    "# 实例化一个ChatMistralAI类：然后设置其Mistral API_KEY；\n",
    "llm = ChatMistralAI(api_key=api_key)\n",
    "print(llm)\n",
    "\n",
    "# 做一个输出\n",
    "output = llm.invoke(messages)\n",
    "print(output.content) # 其返回结果也是一个Message对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Babylon是一家位于 Израël 的技术公司，专注于自然语言处理（NLP）和人工智能（AI）领域的研究和产品开发。公司最知名的产品是Babylon Translator，一款使用深度学习技术的移动和网页翻译工具，支持超过77种语言。Babylon也提供其他基于NLP和AI的产品，如语言学习应用、智能问答系统和商务智能工具。\n",
      "\n",
      "Babylon公司还致力于通过开放API和SDK等方式，将其NLP和AI技术集成到第三方应用和平台中，以扩大其服务范围。 Babylon公司自2016年以来已获得多项技术奖项，并备受业内关注。\n",
      "\n",
      "另外， Babylon 也是一种3D 引擎，用于游戏和虚拟现实等领域的应用，该引擎基于 WebGL 和 WebAssembly 等 Web 技术，并提供了跨平台的支持。 Babylon.js 是 Babylon 引擎的开源实现，可以在网页中直接使用。该引擎提供了丰富的功能，包括物理引擎、动画系统、材质编辑器等，并且支持大多数主流的 3D 模型格式。\n"
     ]
    }
   ],
   "source": [
    "# 使用MistralAI 的 LLM\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "# 读取本地/项目的环境变量。\n",
    "# find_dotenv()寻找并定位.env文件的路径\n",
    "# load_dotenv()读取该.env文件，并将其中的环境变量加载到当前的运行环境中  \n",
    "_ = load_dotenv(find_dotenv()); # 如果环境变量是全局的，这行代码可以省略\n",
    "\n",
    "# 获取环境变量 OPENAI_API_KEY\n",
    "api_key = os.environ['MISTRAL_API_KEY']\n",
    "\n",
    "# 实例化一个ChatMistralAI类：然后设置其Mistral API_KEY；\n",
    "llm = ChatMistralAI(api_key=api_key)\n",
    "# 做一个输出\n",
    "output = llm.invoke(\"介绍一下什么是Babylon?\") # 既可以输入打包好的Message对象，也可以输入字符串\n",
    "\n",
    "print(output.content) # 其返回结果也是一个Message对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "    \"task_name\": \"在Cesium中集成Babylon\",\n",
      "    \"task_type\": \"编程任务\",\n",
      "    \"task_content\": \"需要研究并实现在Cesium中集成Babylon的方法，包括但不限于环境搭建、代码集成、调试等工作。\",\n",
      "    \"task_time\": \"2022-03-15T10:30:00Z\",\n",
      "    \"task_status\": \"未开始\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mistral实现原生解析器实现\n",
    "# 导入环境变量\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "# 从当前目录中的 .env 文件加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "model = \"mistral-large-latest\"\n",
    "\n",
    "# 正则提取任务\n",
    "def parse_task(content): # 从模型生成中字符串匹配提取生成的代码\n",
    "    pattern = r'```task(.*?)```'  # 使用非贪婪匹配\n",
    "    match = re.search(pattern, content, re.DOTALL)\n",
    "    task = match.group(1) if match else content\n",
    "    # task = json.loads(task,strict=False)  # 转换为json格式\n",
    "    return task\n",
    "\n",
    "user_requirement = \"如何在Cesium中集成Babylon？\"\n",
    "prompt :str = f\"\"\"\n",
    "        您是一名任务分析师，您的任务是理解用户需求、并分析和归纳用户意图，生成任务报告。\n",
    "        你要生成的内容要包裹在```task```中，包含的字段有任务名、任务类型、任务内容、任务发布时间、任务完成状态，如下案例：\n",
    "            \"task_name\":\"xxx\",\n",
    "            \"task_type\":\"xxx\",\n",
    "            \"task_content\":\"xxx\",\n",
    "            \"task_time\":\"xxx\",\n",
    "            \"task_status\":\"xxx\"\n",
    "        生成的内容是一个json格式 用大括号json格式扩住,并将将生成的情报信息包裹在```task```中，要求使用中文、完整且精炼的语言进行描述。\n",
    "        好的，请根据以下用户输入生成任务信息,严格中文输出：\n",
    "        {user_requirement}\n",
    "        \"\"\"\n",
    "# 创建模型\n",
    "client = MistralClient(api_key=api_key)\n",
    "\n",
    "# 模型输入\n",
    "chat_response = client.chat(\n",
    "    model=model,\n",
    "    messages=[ChatMessage(role=\"user\", content=prompt)]\n",
    ")\n",
    "\n",
    "# 从模型输出解析任务JSON\n",
    "task = parse_task(chat_response.choices[0].message.content)\n",
    "print(task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "    \"task_name\": \"Cesium中集成Babylon\",\n",
      "    \"task_type\": \"技术集成\",\n",
      "    \"task_content\": \"研究并实施在Cesium框架中集成Babylon引擎的方案，确保功能兼容性和性能优化。\",\n",
      "    \"task_time\": \"2023-11-01\",\n",
      "    \"task_status\": \"进行中\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# deepseek 原生实现结构化输出\n",
    "# 格式化输出内容\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "base_url = os.getenv('BASE_URL')\n",
    "\n",
    "\n",
    "# 正则提取任务\n",
    "def parse_task(content): # 从模型生成中字符串匹配提取生成的代码\n",
    "    pattern = r'```task(.*?)```'  # 使用非贪婪匹配\n",
    "    match = re.search(pattern, content, re.DOTALL)\n",
    "    task = match.group(1) if match else content\n",
    "    # task = json.loads(task,strict=False)  # 转换为json格式\n",
    "    return task\n",
    "\n",
    "user_requirement = \"如何在Cesium中集成Babylon？\"\n",
    "prompt :str = f\"\"\"\n",
    "        您是一名任务分析师，您的任务是理解用户需求、并分析和归纳用户意图，生成任务报告。\n",
    "        你要生成的内容要包裹在```task```中，包含的字段有任务名、任务类型、任务内容、任务发布时间、任务完成状态，如下案例：\n",
    "            \"task_name\":\"xxx\",\n",
    "            \"task_type\":\"xxx\",\n",
    "            \"task_content\":\"xxx\",\n",
    "            \"task_time\":\"xxx\",\n",
    "            \"task_status\":\"xxx\"\n",
    "        生成的内容是一个json格式 用大括号json格式扩住,并将将生成的情报信息包裹在```task```中，要求使用中文、完整且精炼的语言进行描述。\n",
    "        好的，请根据以下用户输入生成任务信息,严格中文输出：\n",
    "        {user_requirement}\n",
    "        \"\"\"\n",
    "client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"GIS开发全栈工程师\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    stream=False\n",
    ")\n",
    "task = parse_task(response.choices[0].message.content)\n",
    "print(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task_name': 'Babylon Integration in Cesium', 'task_type': 'Integration', 'task_content': 'This task involves integrating Babylon into Cesium for 3D visualization.', 'task_time': '2023-03-01', 'task_status': 'In Progress'}\n"
     ]
    }
   ],
   "source": [
    "# 导入所需的库和模块\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "# 获取环境变量 MISTRAL_API_KEY\n",
    "api_key = os.environ['MISTRAL_API_KEY']\n",
    "\n",
    "# 初始化一个 ChatMistralAI 模型实例，并设置温度为 0\n",
    "llm = ChatMistralAI(temperature=0,api_key=api_key)\n",
    "\n",
    "# 定义一个名为 Task 的 Pydantic 模型，用于表示任务数据结构\n",
    "# 该模型包含五个字段：task_name、task_type、task_content、task_time 和 task_status\n",
    "class Task(BaseModel):\n",
    "    task_name: str = Field(description=\"分析任务，得到任务名称\")\n",
    "    task_type: str = Field(description=\"分析任务，得到任务的类型\")\n",
    "    task_content: str = Field(description=\"分析任务，得到任务的内容\")\n",
    "    task_time: str = Field(description=\"分析任务，得到任务的时间\")\n",
    "    task_status: str = Field(description=\"分析任务，得到任务的状态,完成或未完成\")\n",
    "\n",
    "# 定义一个任务查询字符串\n",
    "task_query = \"如何在Cesium中集成Babylon？\"\n",
    "\n",
    "# 初始化一个 JsonOutputParser 实例，用于解析模型生成的 JSON 输出\n",
    "# 并将其转换为 Task 模型实例\n",
    "parser = JsonOutputParser(pydantic_object=Task)\n",
    "\n",
    "# 定义一个 PromptTemplate 实例，用于生成提示字符串\n",
    "# 该模板包含一个查询变量，用于在提示字符串中插入用户输入的查询\n",
    "# 该模板还包含一个格式化说明变量，用于在提示字符串中插入解析器生成的格式化说明\n",
    "prompt = PromptTemplate(\n",
    "    template=\"根据用户输入的问题得到任务JSON.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# 创建一个管道，将提示、模型和解析器连接在一起\n",
    "# 当调用该管道时，它将生成一个提示字符串，将其发送到模型以生成响应，\n",
    "# 然后将响应解析为 Task 模型实例\n",
    "task_chain = prompt | llm | parser\n",
    "\n",
    "# 调用管道，使用用户输入的查询来生成任务数据\n",
    "task_data = task_chain.invoke({\"query\": task_query})\n",
    "print(task_data) # 这里得到的是解析过的结果，而不是message对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyjwt in /usr/local/python/3.10.13/lib/python3.10/site-packages (2.8.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyjwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPStatusError",
     "evalue": "Client error '429 Too Many Requests' for url 'https://open.bigmodel.cn/api/paas/v4/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m task_chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m llm \u001b[38;5;241m|\u001b[39m parser\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# 调用管道，使用用户输入的查询来生成任务数据\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m task_data \u001b[38;5;241m=\u001b[39m \u001b[43mtask_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_query\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(task_data) \u001b[38;5;66;03m# 这里得到的是解析过的结果，而不是message对象\u001b[39;00m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/langchain_core/runnables/base.py:2507\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2505\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2506\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2507\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2508\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2509\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:248\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    244\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    245\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    247\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 248\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    258\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:677\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    671\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    675\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    676\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:534\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    533\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 534\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    535\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    536\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    538\u001b[0m ]\n\u001b[1;32m    539\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:524\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    523\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 524\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m         )\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:749\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 749\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    753\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/langchain_community/chat_models/zhipuai.py:335\u001b[0m, in \u001b[0;36mChatZhipuAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mClient(headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m client:\n\u001b[1;32m    334\u001b[0m     response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzhipuai_api_base, json\u001b[38;5;241m=\u001b[39mpayload)\n\u001b[0;32m--> 335\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpx/_models.py:761\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m error_type \u001b[38;5;241m=\u001b[39m error_types\u001b[38;5;241m.\u001b[39mget(status_class, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid status code\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    760\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[0;32m--> 761\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://open.bigmodel.cn/api/paas/v4/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429"
     ]
    }
   ],
   "source": [
    "# 智谱AI的集成到Langchain实现\n",
    "# 导入所需的库和模块\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "import os\n",
    "\n",
    "# 获取环境变量 GLM_API_KEY\n",
    "api_key = os.environ['ZHIPUAI_API_KEY']\n",
    "\n",
    "# 初始化一个 ChatMistralAI 模型实例，并设置温度为 0\n",
    "llm = ChatZhipuAI(model=\"glm-4\",temperature=0.5,api_key=api_key)\n",
    "\n",
    "# 定义一个名为 Task 的 Pydantic 模型，用于表示任务数据结构\n",
    "# 该模型包含五个字段：task_name、task_type、task_content、task_time 和 task_status\n",
    "class Task(BaseModel):\n",
    "    task_name: str = Field(description=\"分析任务，得到任务名称\")\n",
    "    task_type: str = Field(description=\"分析任务，得到任务的类型\")\n",
    "    task_content: str = Field(description=\"分析任务，得到任务的内容\")\n",
    "    task_time: str = Field(description=\"分析任务，得到任务的时间\")\n",
    "    task_status: str = Field(description=\"分析任务，得到任务的状态,完成或未完成\")\n",
    "\n",
    "# 定义一个任务查询字符串\n",
    "task_query = \"如何在Cesium中集成Babylon？\"\n",
    "\n",
    "# 初始化一个 JsonOutputParser 实例，用于解析模型生成的 JSON 输出\n",
    "# 并将其转换为 Task 模型实例\n",
    "parser = JsonOutputParser(pydantic_object=Task)\n",
    "\n",
    "# 定义一个 PromptTemplate 实例，用于生成提示字符串\n",
    "# 该模板包含一个查询变量，用于在提示字符串中插入用户输入的查询\n",
    "# 该模板还包含一个格式化说明变量，用于在提示字符串中插入解析器生成的格式化说明\n",
    "prompt = PromptTemplate(\n",
    "    template=\"根据用户输入的问题得到任务JSON.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# 创建一个管道，将提示、模型和解析器连接在一起\n",
    "# 当调用该管道时，它将生成一个提示字符串，将其发送到模型以生成响应，\n",
    "# 然后将响应解析为 Task 模型实例\n",
    "task_chain = prompt | llm | parser\n",
    "\n",
    "# 调用管道，使用用户输入的查询来生成任务数据\n",
    "task_data = task_chain.invoke({\"query\": task_query})\n",
    "print(task_data) # 这里得到的是解析过的结果，而不是message对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 封装Mistral Embedding \n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.pydantic_v1 import BaseModel, root_validator\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class MistralAIEmbeddings(BaseModel, Embeddings):\n",
    "    \"\"\"`MistralAI Embeddings` embedding models.\"\"\"\n",
    "\n",
    "    client: Any\n",
    "    \"\"\"`mistralai.MistralClient`\"\"\"\n",
    "\n",
    "    @root_validator()\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        实例化MistralClient为values[\"client\"]\n",
    "\n",
    "        Args:\n",
    "            values (Dict): 包含配置信息的字典，必须包含 client 的字段.\n",
    "\n",
    "        Returns:\n",
    "            values (Dict): 包含配置信息的字典。如果环境中有mistralai库，则将返回实例化的MistralClient类；否则将报错 'ModuleNotFoundError: No module named 'mistralai''.\n",
    "        \"\"\"\n",
    "        from mistralai.client import MistralClient\n",
    "        api_key = os.getenv('MISTRAL_API_KEY')\n",
    "        if not api_key:\n",
    "            raise ValueError(\"MISTRAL_API_KEY is not set in the environment variables.\")\n",
    "        values[\"client\"] = MistralClient(api_key=api_key)\n",
    "        return values\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        生成输入文本的 embedding.\n",
    "\n",
    "        Args:\n",
    "            texts (str): 要生成 embedding 的文本.\n",
    "\n",
    "        Return:\n",
    "            embeddings (List[float]): 输入文本的 embedding，一个浮点数值列表.\n",
    "        \"\"\"\n",
    "        embeddings = self.client.embeddings(\n",
    "            model=\"mistral-embed\",\n",
    "            input=[text]\n",
    "        )\n",
    "        return embeddings.data[0].embedding\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        生成输入文本列表的 embedding.\n",
    "\n",
    "        Args:\n",
    "            texts (List[str]): 要生成 embedding 的文本列表.\n",
    "\n",
    "        Returns:\n",
    "            List[List[float]]: 输入列表中每个文档的 embedding 列表。每个 embedding 都表示为一个浮点值列表。\n",
    "        \"\"\"\n",
    "        return [self.embed_query(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以侧视雷达具有斜距投影的性质。其工作方式为平面扫描和圆锥扫描。3\n",
      "电磁波相关\n",
      "1、电磁波\n",
      "根据麦克斯韦电磁场理论,变化的电场能够在它的周围引起变化的\n",
      "磁场,这个变化的磁场又在较远的区域内引起新的变化电场,并在更远的\n",
      "区域内引起新的变化磁场.这种变化的电场和磁场交替产生,以有限的速\n",
      "度由近及远在空间内传播的过程称为电磁波。2、绝对黑体\n",
      "如果物体对于任何波长的电磁辐射都全部吸收，则这个物体是绝对\n",
      "-----------------\n",
      "\n",
      "电磁波通常以频率f、波长λ来进行描述，它们的关系可以表示为\n",
      "-----------------\n",
      "\n",
      "涉。10、衍射\n",
      "光通过有限大小的障碍物时偏离直线路径的现象成为光的衍射。11、电磁波谱\n",
      "新中地教育不同电磁波由不同波源产生，如果按照电磁波在真空中传播的波长\n",
      "或频率按递增或递减的顺序就能得到电磁波谱图。12、散射\n",
      "电磁波在传播过程中，遇到小微粒而使传播方向发生改变，并向各\n",
      "个方向散开，称为散射。\n",
      "米氏（Mie）散射：如果介质中不均匀颗粒与入射波长同数量级，\n",
      "发生米氏散射。\n",
      "-----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载向量数据库\n",
    "# 使用向量数据库进行检索\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 实例化Embedding模型\n",
    "embeddings_model = MistralAIEmbeddings()\n",
    "# 加载向量数据库\n",
    "loaded_db = FAISS.load_local(\"./db/GIS_db\", embeddings_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "\n",
    "# 计算相似度并检索最相似的文档\n",
    "query = \"电磁辐射\"\n",
    "docs = loaded_db.similarity_search(query, k=3) # 相似度最高的前3个chunk\n",
    "\n",
    "# 输出检索结果\n",
    "for doc in docs:\n",
    "    print(doc.page_content+\"\\n-----------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cesium 和 Babylon.js 是两个 popular 的 JavaScript 库，分别用于创建 3D 地球和 3D 游戏。Cesium 的优势在于其提供的大量 geospatial 功能和数据集，而 Babylon.js 则更适合于创建丰富的 3D 场景和动画效果。\n",
      "\n",
      "虽然 Cesium 和 Babylon.js 是两个独立的库，但是它们可以通过 WebGL 渲染器进行集成。以下是一种可能的集成方法：\n",
      "\n",
      "1. 创建 Cesium 视图：\n",
      "\n",
      "```javascript\n",
      "var viewer = new Cesium.Viewer('cesiumContainer');\n",
      "```\n",
      "\n",
      "2. 创建 Babylon.js 场景：\n",
      "\n",
      "```javascript\n",
      "var canvas = viewer.scene.canvas;\n",
      "var engine = new BABYLON.Engine(canvas, true);\n",
      "var scene = new BABYLON.Scene(engine);\n",
      "```\n",
      "\n",
      "3. 将 Babylon.js 场景添加到 Cesium 视图中：\n",
      "\n",
      "```javascript\n",
      "var camera = viewer.camera;\n",
      "var babylonCamera = new BABYLON.ArcRotateCamera(\"babylonCamera\", \n",
      "    camera.position.x, camera.position.y, camera.position.z, \n",
      "    camera.direction.x, camera.direction.y, camera.direction.z, scene);\n",
      "babylonCamera.attachControl(canvas, true);\n",
      "\n",
      "var renderLoop = function() {\n",
      "    scene.render();\n",
      "    requestAnimationFrame(renderLoop);\n",
      "};\n",
      "renderLoop();\n",
      "```\n",
      "\n",
      "上述代码创建了一个 Cesium 视图，并在其上添加了一个 Babylon.js 场景。Babylon.js 场景使用 Cesium 相机进行渲染，并在每帧渲染时调用 Babylon.js 的 `render` 函数。\n",
      "\n",
      "需要注意的是，Cesium 和 Babylon.js 使用不同的坐标系统，因此在集成时需要进行转换。此外，Cesium 的渲染优先级较高，因此 Babylon.js 场景可能会被 Cesium 的地球遮挡。\n",
      "\n",
      "总的来说，将 Cesium 和 Babylon.js 集成在一起可以提供更丰富的 3D 视图和动画效果，但是需要注意它们之间的差异和限制。\n"
     ]
    }
   ],
   "source": [
    "# 继续使用Mistral\n",
    "# 导入所需的库和模块\n",
    "import os\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "# 获取环境变量 MISTRAL_API_KEY\n",
    "api_key = os.environ['MISTRAL_API_KEY']\n",
    "\n",
    "# 初始化一个 ChatMistralAI 模型实例，并设置温度为 0\n",
    "llm = ChatMistralAI(temperature=0,api_key=api_key)\n",
    "\n",
    "\n",
    "# 定义一个任务查询字符串\n",
    "task_query = \"如何在Cesium中集成Babylon？中文回答\"\n",
    "\n",
    "\n",
    "# 调用管道，使用用户输入的查询来生成任务数据\n",
    "result = llm.invoke(task_query)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建检索问答链\n",
    "import os\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "# 实例化Embedding模型\n",
    "embeddings_model = MistralAIEmbeddings()\n",
    "# 加载向量数据库\n",
    "loaded_db = FAISS.load_local(\"./db/GIS_db\", embeddings_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "# 获取环境变量 MISTRAL_API_KEY\n",
    "api_key = os.environ['MISTRAL_API_KEY']\n",
    "\n",
    "# 初始化一个 ChatMistralAI 模型实例，并设置温度为 0\n",
    "llm = ChatMistralAI(temperature=0,api_key=api_key)\n",
    "\n",
    "\n",
    "template = \"\"\"使用以下上下文来回答最后的问题。如果你不知道答案或者不确定结果，只需要你不知道，不要试图编造答\n",
    "案。最多使用三句话。尽量使答案简明扼要。总是在回答的最后说“谢谢你的提问！要求使用中文回答”。\n",
    "{context}\n",
    "问题: {question}\n",
    "\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\",\"question\"],\n",
    "                                 template=template)\n",
    "\n",
    "\n",
    "# 构建检索问答链\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=loaded_db.as_retriever(),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={\"prompt\":QA_CHAIN_PROMPT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大模型+知识库后回答 question_1 的结果：\n",
      "GIS，全称地理信息系统，是一个在计算机硬件和软件的支持下，以地理空间数据库为基础，以具有空间内涵的地理数据为处理对象的计算机系统。它可以用来收集、存储、显示、处理、分析和输出地理信息，为规划、管理和决策提供信息来源和技术支持。GIS是一个空间型信息系统，也是城市基础设施之一，常用于城市规划、管理和科学决策中。\n",
      "\n",
      "谢谢你的提问！\n",
      "大模型+知识库后回答 question_2 的结果：\n",
      "王司徒是本问题所提供的背景知识中没有提到的人，因此我不知道他是谁。\n",
      "\n",
      "谢谢你的提问！\n"
     ]
    }
   ],
   "source": [
    "# 测试结果\n",
    "question_1 = \"什么是GIS？\"\n",
    "question_2 = \"王司徒是谁？\"\n",
    "result = qa_chain({\"query\": question_1})\n",
    "print(\"大模型+知识库后回答 question_1 的结果：\")\n",
    "print(result[\"result\"])\n",
    "result = qa_chain({\"query\": question_2})\n",
    "print(\"大模型+知识库后回答 question_2 的结果：\")\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIS (Geographic Information System) 是一种能够收集、存储、管理和分析地理数据或信息，并将这些数据可视化呈现出来的系统。GIS 可以将各种类型的数据，如地图、图像、环境数据、社会经济数据等，与地理位置信息相关联，从而帮助用户进行空间分析、问题定位和决策支持等工作。GIS 常用于城市规划、环境保护、交通运输、能源管理、自然资源探 exploitation、地理学研究等领域。\n",
      "------------------------\n",
      "\n",
      "王司徒是中国古代名人，字仲之，是东汉 dynasty 时期的著名文学家、政治家。他出生在今天的江苏省常州市，是东汉初期名家、文学家、政治家 Yang Xiong 的孙子。\n",
      "\n",
      "王司徒以才华 bright 之名，在文学、历史、哲学等领域都有很大的成就。他著名的作品包括《说苑》、《离骚》、《论语注》等。在政治上，他也是一位坚定的理想家，反对腐败和官僚主义，并提出了改革建设的建议。\n",
      "\n",
      "王司徒被视为中国文化历史上的一个重要人物，他的思想和文学作品对中国文化有着深远的影响。\n"
     ]
    }
   ],
   "source": [
    "# 对比有RAG和无RAG的回答结果\n",
    "prompt_template = \"\"\"请回答下列问题:\n",
    "                            {}\"\"\".format(question_1)\n",
    "### 基于大模型的问答\n",
    "print(llm.predict(prompt_template)+'\\n------------------------\\n')\n",
    "\n",
    "\n",
    "prompt_template1 = \"\"\"请回答下列问题:\n",
    "                            {}\"\"\".format(question_2)\n",
    "### 基于大模型的问答\n",
    "print(llm.predict(prompt_template1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "病等。这些都与地理因素有关。不论是从事一种新的职业，:还是寻找生长香蕉的 最合适的土壤，是为救护车计算最佳的行车路线，这些本地问题也都有地理因素。 地图制作和地理分析已不是新鲜事，但 GIS 执行这些任务比传统的手工方法更好 更快。而且，在 GIS 技术出现之前，只有很少的人具有利用地理信息来帮助做出 决定和解决问题的能力。今天， GIS 已是一个全球拥有数十万的人员和数十亿美 元的产业。\n",
      "\n",
      "个公司来说具体的操作实践又是独特的。GIS 如何工作：GIS 就是用来存储有关世界的信息，这些信息是可以通过地理关系连接在一起的所 有主题层集合。这个简单却非常有力和通用的概念，对于解决许多真实世界的问题 具有无价的作用，这些问题包括：跟踪传输工具、记录计划的详细资料，:模拟全 球的大气循环等。地理参考系统：地理信息包含有明确的地理参照系统，例如经度和纬度坐标，或者是国家网格坐\n",
      "\n",
      "【参考答案】（1）联系\n",
      "地图是记录地理信息的一种图形语言形式，从历史发展的角度看，地理信息系统脱胎于地图，并成为地图信息的又一种新的载体形式，尽管GIS给制图业注入了新鲜血液，但GIS也离不开地图，空间分析需要地图作为基础，在地图上进行一系列操作，最终的成果依然是在地图上表现，故说地图既是空间分析输入数据的来源，又是输出数据的一种形式。（2）差异\n",
      "\n",
      "Database）为基础，以具有空间内涵的地理数据为处理对象，运用系统工程和信息科学的理论，采集、存储、显示、处理、分析、输出地理信息的计算机系统，为规划、管理和决策提供信息来源和技术支持。简单地说，GIS就是研究如何利用计算机技术来管理和应用地球表面的空间信息，它是由计算机硬件、软件、地理数据和人员组成的有机体，采用地理模型分析方法，适时提供多种空间的和动态的地理信息，为地理研究和地理决策服务\n",
      "Human: 如何学习GIS呢？要求中文回答\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "GIS，或地理信息系统，是一个计算机系统，用于存储、显示、处理和分析可通过地理关系连接在一起的世界信息的主题层集合。它常被用于解决各种真实世界的问题，例如跟踪传输工具、记录计划的详细信息，模拟全球大气循环等。地图是GIS的基础，GIS为制图业注入了新鲜血液，但也离不开地图。\n",
      "\n",
      "学习GIS需要具备相关的背景知识，包括计算机科学、地理学和统计学等。可以通过以下几种方式学习GIS：\n",
      "\n",
      "1. 在校学习：选择专业是地理信息科学、地理学或计算机科学的大学，并在课程中学习GIS。\n",
      "2. 在线学习：参加在线课程或MOOC（大规模在线开放课程），例如Esri的GIS专业证书课程、Coursera的GIS课程等。\n",
      "3. 自学：通过阅读GIS相关书籍、手册和在线资源来学习GIS。\n",
      "4. 实习：通过实习或工作来获取GIS的实际操作经验。\n",
      "\n",
      "学习GIS需要掌握相关的软件，例如ArcGIS、QGIS、MapInfo等。同时，还需要了解地理参考系统、空间分析、地理数据处理等知识。\n"
     ]
    }
   ],
   "source": [
    "# 添加历史对话功能\n",
    "import os\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# 封装Mistral Embedding \n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.pydantic_v1 import BaseModel, root_validator\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class MistralAIEmbeddings(BaseModel, Embeddings):\n",
    "    \"\"\"`MistralAI Embeddings` embedding models.\"\"\"\n",
    "\n",
    "    client: Any\n",
    "    \"\"\"`mistralai.MistralClient`\"\"\"\n",
    "\n",
    "    @root_validator()\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        实例化MistralClient为values[\"client\"]\n",
    "\n",
    "        Args:\n",
    "            values (Dict): 包含配置信息的字典，必须包含 client 的字段.\n",
    "\n",
    "        Returns:\n",
    "            values (Dict): 包含配置信息的字典。如果环境中有mistralai库，则将返回实例化的MistralClient类；否则将报错 'ModuleNotFoundError: No module named 'mistralai''.\n",
    "        \"\"\"\n",
    "        from mistralai.client import MistralClient\n",
    "        api_key = os.getenv('MISTRAL_API_KEY')\n",
    "        if not api_key:\n",
    "            raise ValueError(\"MISTRAL_API_KEY is not set in the environment variables.\")\n",
    "        values[\"client\"] = MistralClient(api_key=api_key)\n",
    "        return values\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        生成输入文本的 embedding.\n",
    "\n",
    "        Args:\n",
    "            texts (str): 要生成 embedding 的文本.\n",
    "\n",
    "        Return:\n",
    "            embeddings (List[float]): 输入文本的 embedding，一个浮点数值列表.\n",
    "        \"\"\"\n",
    "        embeddings = self.client.embeddings(\n",
    "            model=\"mistral-embed\",\n",
    "            input=[text]\n",
    "        )\n",
    "        return embeddings.data[0].embedding\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        生成输入文本列表的 embedding.\n",
    "\n",
    "        Args:\n",
    "            texts (List[str]): 要生成 embedding 的文本列表.\n",
    "\n",
    "        Returns:\n",
    "            List[List[float]]: 输入列表中每个文档的 embedding 列表。每个 embedding 都表示为一个浮点值列表。\n",
    "        \"\"\"\n",
    "        return [self.embed_query(text) for text in texts]\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",  # 与 prompt 的输入变量保持一致。\n",
    "    return_messages=True  # 将以消息列表的形式返回聊天记录，而不是单个字符串\n",
    ")\n",
    "\n",
    "# 实例化Embedding模型\n",
    "embeddings_model = MistralAIEmbeddings()\n",
    "# 加载向量数据库\n",
    "loaded_db = FAISS.load_local(\"./db/GIS_db\", embeddings_model, allow_dangerous_deserialization=True)\n",
    "# 获取环境变量 MISTRAL_API_KEY\n",
    "api_key = os.environ['MISTRAL_API_KEY']\n",
    "\n",
    "# 初始化一个 ChatMistralAI 模型实例，并设置温度为 0\n",
    "llm = ChatMistralAI(temperature=0,api_key=api_key)\n",
    "# 构建新的问答链，使用带有记忆的提示模板\n",
    "retriever=loaded_db.as_retriever()\n",
    "\n",
    "#构建对话问答链\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    ")\n",
    "question = \"如何学习GIS呢？要求中文回答\"\n",
    "result = qa({\"question\": question})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: 如何学习GIS呢？要求中文回答\n",
      "Assistant: GIS，或地理信息系统，是一个计算机系统，用于存储、显示、处理和分析可通过地理关系连接在一起的世界信息的主题层集合。它常被用于解决各种真实世界的问题，例如跟踪传输工具、记录计划的详细信息，模拟全球大气循环等。地图是GIS的基础，GIS为制图业注入了新鲜血液，但也离不开地图。\n",
      "\n",
      "学习GIS需要具备相关的背景知识，包括计算机科学、地理学和统计学等。可以通过以下几种方式学习GIS：\n",
      "\n",
      "1. 在校学习：选择专业是地理信息科学、地理学或计算机科学的大学，并在课程中学习GIS。\n",
      "2. 在线学习：参加在线课程或MOOC（大规模在线开放课程），例如Esri的GIS专业证书课程、Coursera的GIS课程等。\n",
      "3. 自学：通过阅读GIS相关书籍、手册和在线资源来学习GIS。\n",
      "4. 实习：通过实习或工作来获取GIS的实际操作经验。\n",
      "\n",
      "学习GIS需要掌握相关的软件，例如ArcGIS、QGIS、MapInfo等。同时，还需要了解地理参考系统、空间分析、地理数据处理等知识。\n",
      "Follow Up Input: 什么意思？中文回答\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "存储、显示、处理、分析、输出地理信息的计算机系统，为规划、管理和决策提供 信息来源和技术支持。简单地说， GIS 就是研究如何利用计算机技术来管理和应 用地球表面的空间信息，它是由计算机硬件、软件、地理数据和人员组成的有机 体， 采用地理模型分析方法，适时提供多种空间的和动态的地理信息，为地理研 究和地理决策服务的计算机技术系统。地理信息系统属于空间型信息系统。**\n",
      "\n",
      "究和地理决策服务的计算机技术系统。地理信息系统属于空间型信息系统。** 从技术角度解释GIS**地理信息科学(Geographic Information\n",
      "\n",
      "Database）为基础，以具有空间内涵的地理数据为处理对象，运用系统工程和信息科学的理论，采集、存储、显示、处理、分析、输出地理信息的计算机系统，为规划、管理和决策提供信息来源和技术支持。简单地说，GIS就是研究如何利用计算机技术来管理和应用地球表面的空间信息，它是由计算机硬件、软件、地理数据和人员组成的有机体，采用地理模型分析方法，适时提供多种空间的和动态的地理信息，为地理研究和地理决策服务\n",
      "\n",
      "地理信息系统GIS地理信息系统（Geographic Information System，GIS）：作为信息技术的一种， 是在计算机硬、软件的支持下，以地理空间数据库（ Geospatial Database ）为基础 ， 以具有空间内涵的地理数据为处理对象，运用系统工程和信息科学的理论，采集、 存储、显示、处理、分析、输出地理信息的计算机系统，为规划、管理和决策提供\n",
      "Human: 可以请你解释一下\"GIS\"和\"地理参考系统、空间分析、地理数据处理等知识\"的含义吗？（中文回答）\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "GIS, or Geographic Information System, is a computer-based system that stores, displays, processes, analyzes, and outputs geographic information. It is used for planning, managing, and decision-making and is composed of computer hardware, software, geographic data, and personnel. It utilizes geographic model analysis methods to provide timely, various spatial and dynamic geographic information for geographic research and decision-making.\n",
      "\n",
      "A geographic reference system, also known as a coordinate reference system, is a system used to locate places on the earth's surface. It includes a coordinate system, a datum, and a prime meridian.\n",
      "\n",
      "Spatial analysis is the process of examining the locations, attributes, and relationships of geographic features. It involves the use of statistical and mathematical methods to understand patterns and trends in geographic data.\n",
      "\n",
      "Spatial data processing is the manipulation and transformation of geographic data to make it suitable for analysis and visualization. This can include tasks such as data cleaning, data integration, and data transformation.\n",
      "\n",
      "In summary, GIS is a system for managing and applying spatial information using computer technology, while geographic reference systems, spatial analysis, and spatial data processing are all important components of the broader field of geographic information science.\n"
     ]
    }
   ],
   "source": [
    "question = \"什么意思？中文回答\"\n",
    "result = qa({\"question\": question})\n",
    "print(result['answer'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
